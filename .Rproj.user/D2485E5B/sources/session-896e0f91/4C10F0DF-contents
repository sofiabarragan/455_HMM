--- 
title: "Hidden Markov Models & their Applications to Statistical Genetics"
author: "Sofia Barragan, Spring 2021, Macalester College"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    highlight: tango
    config:
      sharing:
        facebook: false
        twitter: false
always_allow_html: true
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: sofiabarragan/455_HMM
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = "center", error = FALSE)
```

# Welcome {-}

This bookdown on discrete Hidden Markov Models & their applications to statistical genetics is my capstone! I made this for my Mathematical Statistics course taught by [Kelsey Grinde](http://kegrinde.github.io). Big thanks to her for her guidance and help.

Content was written and gathered by [Sofia Barragan](https://www.sofiabarragan.com) with appropriate citations for print materials. Wherever possible, I try to provide direct link citations for any digital materials or resources. 

Embedded Youtube videos are under the sole ownership of their linked creator. 

<br>
**Note:** This is a very brief primer & I assume minimal mathematical background.

<br>
<br>
![](images/cc_license_button.png)

This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).

```{r include=FALSE}
install.packages("bookdown",repos = "http://cran.us.r-project.org")
library(bookdown)
# or the development version
# devtools::install_github("rstudio/bookdown")
```

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```



<!--chapter:end:index.Rmd-->

# Mathematical Intuition

Markov Chains are cool! Hidden Markov Models are also cool, but require more preparation! In this section, we'll go through conditional probabilities & set up the basis to study Hidden Markov Models by getting comfortable with chains first.

\

---

## Conditional Probability & Bayes Rule

### Basic Concepts {-}

Probability, as a field, formalizes how we predict events with some equally beautiful & ugly notation, intuitive concepts, and complex mathematical principles. But the premise is simple: by ascribing a numeric value to the outcomes of an event, we can abstract the real world and study it with math. 

The process of ascribing numeric values to the outcome of an event is called mapping & by mapping all possible probabilities of an event's outcomes, we create a **random variable**.

> **NOTE:** This can be confusing! The "random" part of the word doesn't mean all outcomes have an equal chance of happening; really, it means that within an event, there are multiple possible outcomes.

\

**Example: Weather & Temperature**

Let's say that the weather is the event, $W$, whose only **outcomes** are sunny ($w_s$), rainy ($w_r$) or cloudy($w_c$). 
By mapping numeric values (e.g., probabilities) to the outcomes, we can turn $W$ into a random variable. Below, we list all mappings in the **probability mass function**, $p_{(W)}(w)$.

\begin{align*} 
p_{(W)}(w)  &= \begin{cases}
    0.5, & \text{for } w_s \\
    0.2, & \text{for } w_r \\
    0.3, & \text{for } w_c \\
    0.0, & \text{otherwise}
\end{cases}
\end{align*} 

\

> **NOTE:** The sub-probability of *all* probability mass functions must sum to 1.

\

Now, let's see that the daily temperature (F°) is the event $T$. Normally, we could say that $T$ follows a normal distribution with $\mu = 75 \textbf{ F°}$ and $\sigma^2 = 625 \textbf{ F°}$, since its a continuous variable. So, $T \sim N(75, 625)$ and would look something like this

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE, fig.width=6, fig.height=3, fig.align = "center"}
library(tidyverse)
ggplot(data = data.frame(x = c(-25, 175)), aes(x)) +
  stat_function(fun = dnorm, 
                n = 1000,
                args = list(mean = 75, sd = 25), 
                color="black", size=1.5) + 
  labs(y="Probabilities", title="Temperature (F°)", x="")+
  scale_x_continuous(breaks = seq(0,150, by=25))+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

\

Instead, let's say that $T$ is a discrete random variable whose only potential outcomes are cold ($t_c$), fresh ($t_n$), or hot ($t_h$). Then, $T$ has the probability mass function

\begin{align*} 
p_{(T)}(t)  &= \begin{cases}
    0.155, & \text{for } t_c \\
    0.5, & \text{for } t_f \\
    0.345, & \text{for } t_h \\
    0.0, & \text{otherwise}
\end{cases}
\end{align*} 

\

Below are two graphs summarizing what we know so far about $W$ and $T$ 

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE}
data <- data.frame(probability = c(0.5, 0.3,0.2), temperature = c("Sunny", "Rainy", "Cloudy"))

p1<-ggplot(data, aes(x=temperature, y=probability, fill=temperature)) +
  geom_col()+
  scale_fill_manual(values=c("#D5D5D3", "#85D4E3",  "#FAD77B"))+
  labs(y="Probabilities", title="Weather", x="")+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE}
data <- data.frame(probability = c(0.155, .5,.345), temperature = c("Cold", "Fresh", "Hot"))

p2 <- ggplot(data, aes(x=temperature, y=probability, fill=temperature)) +
  geom_col()+
  scale_fill_manual(values=c("#3B9AB2", "#00A08A",  "#F1BB7B"))+
  labs(y="", title="Temperature", x="")+
  theme_linedraw()+
    theme(legend.position = "none", plot.title = element_text(hjust=.5))
```

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE,  fig.width=6, fig.height=4/1.2, fig.align = "center"}
library(gridExtra)
grid.arrange(p1,p2, ncol=2)
```

But what happens if the weather depends on the temperature? 

\
\

### Conditional Probabilities {-}

Let's study the two arbitrary events $A$, $B$ & learn some definitions about probability.

**Conditional Probability:** 
The conditional probabilities of $A$'s, given $B$ and $B$, given $A$ are written below.
$$P(A \mid B) \hspace{1 in}P(B \mid A)$$
\

**Independence:** 
We say that the two events, $A$ & $B$ are independent if the conditional probabilities provide us no new-information about either event. So, 

$$P(A \mid B) = P(A) \hspace{1 in}P(B \mid A) = P(B)$$
\

**Joint Probability:** 
The probability of two events, $A$ & $B$ happening at the same time is called a **joint probability** and is typically denoted by $P(A\cap B)$. It is calculated below.

\begin{align*} 
P(A\cap B) &= P(A \mid B) \cdot P(B)\\
&= P(A) \cdot P(B) && \text{if independent}
\end{align*} 

\
\


It's generally true that the weather on a particular day, depends on the temperature. This implies that $W$ and $T$ are conditional events with conditional probabilities. So, 

$$P(W\mid T) \neq P(W)$$

### Bayes' Rule & LOTP {-}

Conceived by Reverend Thomas Bayes in the 18th Century, posthumously published by his friend Richard Price, and then formalized into an equation by Pierre-Simon Laplace, Bayes' Rule is a cornerstone equation in modern statistics & probability^[https://www.bayesrulesbook.com/chapter-1.html#a-quick-history-lesson]. I've written Bayes Rule below^[https://www.bayesrulesbook.com/chapter-2.html]

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}=\frac{P(A)\cdot P(B \mid A)}{P(B)}$$

> **NOTE**: Above you'll notice that we're looking at the probability of $A$ & $B$, divided by the probability of $B$. We are dividing by $P(B)$ to normalize & isolate the probability of $A$, under the conditions we observe $B$.

> **Quick Check:** If $A$ & $B$ are independent, how would you further simplify the numerator of Bayes' Rule?

The denominator of Bayes' Rule, $P(B)$ is called the **marginal probability** of $B$.

The marginal probability can either be

- given 
- computed with the **law of total probability** or (**LOTP**). 

**LOTP** states that

\begin{align*}
P(B) = \sum_{i=1}^n P(B \cap A_i) &= P(B \cap A_1) + P(B\cap A_2) + \dots + P(B\cap A_n) \\
&= P(B \mid A_1)P(A_1) + P(B\mid A_2)P(A_2) + \dots + P(B\mid A_n)P(A_n) \\
\end{align*} 

> **Note:** Looks scary! Really it's like calculating the probability of $B$ in each subcategory of $A$, multiplying by the probability of that sub-$A$, then adding it all together.

\

---

## Markov Chains

**Stochastic processes** are events that have some element of randomness in their outcomes. The amount of 'randomness' & the type of events can vary depending on the context. In turn, studying the properties of stochastic processes often requires many different techniques which go well beyond the boundaries of statistics. And with a litany of applications across so many domains of knowledge, studying stochastic processes also involves many techniques from Physics, Linguistics, Sociology, Public Health, Geography & more. 

> **Note:** There is *some* nuance in the language we use to describe randomness, probabilistic, and stochastic, but it is murky. So, for now, let's stick with the above definition & enjoy some interchangeability between random, stochastic, and probabilistic.

So, this section will only be a tiny snippet of the wide topics covered in studying stochastic processes.

### Intuition {-}

Markov chains are a subclass of stochastic processes that describe partially random events occurring in succession, typically in succession. We will formalize this definition soon, but for now, let's talk about the weather.

**Example: Weather**

Let's ignore the fact that temperature determines weather. Instead, let's assume that we can compute the probability of tomorrow's weather by only looking at today's. This results in three important ideas:

- General weather patterns before today are irrelevant when predicting tomorrow's weather. This is an example of a Markov process, more specifically a Markov Chain.
- The outcomes of the weather are rainy, sunny, or cloudy. These are examples of states.
- The probability of tomorrow's weather depends on whether it was rainy, sunny, or cloudy to day. Meaning there are probabilities associated with tomorrow's events, strictly defined by today's. These are examples of transition probabilities.

Let's be rigorous now!

### Mathematical Definitions {-}

Let $X_i = X_1, \dots, X_{n-1}, X_n$ be a collection of $n$ successively indexed events.

The discrete $X_i$ are a **Markov Chain** if 


- They exhibit a **Markov Property**.
  + Where the probability of some new or predicted event $X_{n+1}=x_{n+1}$ is 

$$P(X_{n+1} = x_{n+1} \mid X_1 = x_1, \dots,  X_{n-1}= x_{n-1}, X_n= x_{n}) = P(X_{n+1} = x_{n+1} \mid X_n = x_n)$$

> **NOTE:** This is similar to the conditional probability of independent events! But instead we specify that prior events $X_1,\dots X_{n-1}$ are independent of the outcome of $X_{n+1}$, but $X_{n}$ is not.

\

- There is some countable set of outcomes called the **State Space** which contains every possible outcome or **State** 
  + The outcomes $x_1, \dots, x_{n-1}, x_{n}, x_{n+1}$ are all common elements of the state space, $\mathbb{S}$.

> **NOTE:** This may seem complicated, but this means we can define what are possible and impossible outcomes.

\


- The probability of changing states is a **Transition Probability** and if we were to write them out for each *state*, they would sum to $1$.
  + Transition Probabilities sum to 1 because they cumulatively define the *probability mass function* of the transition from each state to another.
  + These probabilities can be placed in a **Transition Matrix** where the columns indicate a next state & the rows indicate the current state.  
  
> **NOTE:** The above definitions were cumulatively drawn from the following sources ^[Lay, David C. 2012. “Applications to Markov Chains.” In Linear Algebra and Its Applications, 4th ed., 253–62. Boston: Pearson College Division.] ^[https://setosa.io/ev/markov-chains/]
  
\
  
### Visualizations {-}  
  
**Weather Graphs**

Below we've redefined our weather example as a Markov Chain, using our new definitions, and place it into a Weighted Directed Graph. Isn't she pretty! Note that the *weights* on our arrows correspond to the transition probability associated with that change-arrow. 

![](images/HMM_Diagram_Intuition.png)

Also, check out that Transition Matrix! It's the first one you've looked at, but they can quickly get very complex.

\

**Weather Animations**

To simulate how a Markov Chain of weather behaves, let's animate it! 

In this visualization, there are 10 multicolored dots representing different arbitrary days colored by that particular day's weather. To see how Markov Chains evolve over 15 days, we shuffle them 15 times in a row according to our transition matrix 

|Rainy|Sunny|Cloudy|
|:---:|:---:|:----:|
| .20 | .46 | .34 |
| .00 | .80 | .20 |
| .15 | .50 | .30 |

Written text describes the proportion of weather outcomes following simulation. That is, how many rainy, sunny, or cloudy days happen after a given shuffle.

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE, fig.align = "center"}
library(plotly)
set.seed(200322)
markov2 <- function(n = 10, iter = 15, start_probs = c(.4, .4, .2), 
                    trans_probs = matrix(c(.2, 0, .15, 
                                           .46, .8, .55,
                                           .34, .2, .30), ncol = 3),
                    plot_prob = TRUE) { # Here we have a full non-zero transition matrix.
  #Check to see if probability entries are valid.
  if(sum(start_probs) != 1 | any(start_probs < 0)) 
    stop("start_probs must be non-negative and sum to 1.")
  if(any(apply(trans_probs, 1, sum) != 1))
    stop("trans_probs matrix rows must sum to 1.")
  if(any(trans_probs < 0))
    stop("elements of trans_probs must be non-negative")

  dt <- matrix(NA, nrow = iter, ncol = n) # Initialize matrix to hold iterations
  # Run chain
  for(i in 1:iter) {
    for(j in 1:n) {
      if(i == 1) { # if we're at the beginning of the simulation
        dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = start_probs)
      } else {
        if(dt[i - 1, j] == 0) { # if the previous state was 0
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,1])
        } else if (dt[i - 1, j] == 1) { # if the previous state was 1
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,2])
        } else {
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,3])
        }
      }
    }
  }
  
  # Return chain as dataframe
  return(as.data.frame(dt))
}

df2 <- markov2()


df_long2 <- df2 %>%
  rowid_to_column(var = "iter") %>%
  pivot_longer(cols = V1:V10) %>%
  group_by(name) %>%
  mutate(x = value + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         y = value %% 2 + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         initial = value[1],
         is_0 = ifelse(value == 0, TRUE, FALSE),
         is_1 = ifelse(value == 1, TRUE, FALSE)) %>%
  group_by(iter) %>%
  mutate(prop0 = mean(is_0),
         prop1 = mean(is_1),
         prop2 = 1 - prop0 - prop1) %>%
  ungroup()

anim2 <- df_long2 %>%
  plot_ly(
    x = ~x,
    y = ~y,
    color = ~factor(initial),
    size = 5,
    colors =c("#D5D5D3", "#85D4E3",  "#FAD77B" ),
    frame = ~iter,
    type = 'scatter',
    mode = 'markers',
    showlegend = FALSE
  )
anim2 <- anim2 %>%
  add_text(x = 0, y = .35, text = ~prop0, textfont = list(color = "#D5D5D3", size = 24, opacity = .6)) %>%
  add_text(x = 1, y = 1.25, text = ~prop1, textfont = list(color = "#85D4E3", size = 24, opacity = .6)) %>%
  add_text(x = 2, y = .35, text = ~prop2, textfont = list(color = "#FAD77B", size = 24, opacity = .6))

ax2 <- list(
  zeroline = TRUE,
  showline = TRUE,
  mirror = "ticks",
  showticklabels = FALSE,
  gridcolor = toRGB("white"),
  gridwidth = 2,
  zerolinecolor = toRGB("white"),
  zerolinewidth = 4,
  linecolor = toRGB("black"),
  linewidth = 2,
  title = ""
)

anim2 <- anim2 %>%
  layout(xaxis = ax2, yaxis = ax2) %>%
  animation_opts(redraw = FALSE) %>%
  animation_slider(hide = TRUE) %>%
  animation_button(x = .6, y = .10, showactive = FALSE, label = "Run Simulation") %>%
  config(displayModeBar = FALSE, scrollZoom = FALSE, showTips = FALSE)
  
anim2


```
Look at that! We converged on our initial probability distribution. That isn't necessarily true for all simulations, but it's true for this one.

> **NOTE:** This Markov Chain visualization was designed by [Will Hipson](https://willhipson.netlify.app), a graduate student in Psychology at Carleton University. You can find links for reproduction at his page^[https://willhipson.netlify.app/post/markov-sim/markov_chain/] or check out my GitHub to see my edits.
 

\

---

## Conclusion

We've covered the necessary probability concepts. In the next section, we'll find out out how we can leverage Markov Chains to predict another set of variables, even when we can't see the outcomes of our chain. Prepare for bivariate distributions, many subscripts, and a lot of summation notation!

If you have any lingering questions, I've linked some great YouTube videos that may be helpful below.


## Video Resources {-}

**Conditional Probability**

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ibINrxJLvlM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


**Bayes Rule**

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/XQoLVl31ZfQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

**Markov Chains**

<center>
<iframe width="280" height="157.5" src="https://www.youtube.com/embed/VCyJGp6Enxg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

<iframe width="280" height="157.5" src="https://www.youtube.com/embed/JHwyHIz6a8A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>  

</center>




## References {-}


<!--chapter:end:01-Mathematical-Intuition.Rmd-->

# Frog Families & HMMs

Previously, we wanted to study the weather. Let's up the stakes and now look at how the weather can determine whether or not a cute, infinitely reproducing, frog-family survives the month.

If you were a frog, you wouldn't be able to check the weather before leaving your tree hide-- you wouldn't even be able to read!-- but you do know that when a family member leaves for their daily hop, their chance of living depends on the weather. If the weather is 

- **Sunny:**      they would have a 10% chance of living (and eating a nice bug) with a 90% chance of dying.
- **Cloudy:**     they would have a 75% chance of living (and eating a nice bug) with a 25% chance of dying.
- **Rainy:**      they would have a 98% chance of living (and eating a nice bug) with a 2% chance of dying.

Since we, as non-frog statisticians, know that the weather is a Markov Chain, we can say the survival of this frog-family is actually a Hidden Markov Model. In the following sections, we will establish what that implies theoretically for us & our frog family.

\

---

## Hidden Markov Models 

Hidden Markov Models are another class of probabilistic models that model Markov processes whose outcomes cannot be directly observed, but their dependent observed events can be. Often, we're really interested in these unobserved events and predicting them, so we can work backward and use the observed ones^[https://www.nature.com/articles/nbt1004-1315#citeas].

Intuitively, it's like using the shadow of an animal to guess what it is!

<center>
![Fig 1. Frog Shadow Made by Author](images/frogshadow.png){#id .class width=50% height=50%}
</center>

\


### Properties {-}

**Let**

- $X_i = X_1=x_1, \dots, X_{n-1}=x_{n-1}, X_n =x_n$ be a collection of $n$ successively indexed events.
- $Y_i =  Y_1=y_1, \dots, Y_{n-1}=y_{n-1}, Y_n =y_n$ be a collection of $n$ successively indexed observations.
- $\lambda = (X,Y)$. 

$\lambda$ is a discrete **Hidden Markov Model** if it fulfills the following definitions:

##### Discreteness: {-}

Both $X_i$ and $Y_i$ are sequential, discrete, random variables with the $n$ observations $x_{1,\dots, n}$ and $y_{1,\dots, n}$.
 
> **Note:** Neither $X_i$ nor $Y_i$ *must* be discrete events, but this project is explicitly dedicated to discrete Hidden Markov Models, so we will assume they are. 
 
##### Markov Assumptions: {-}

- The $X_i$ *must* be a Markov Chain. 
   + So, $$P(X_{n} = x_{n} \mid X_1 = x_1, \dots,  X_{n-1}= x_{n-1}, X_n= x_{n}) = P(X_{n} = x_{n} \mid X_n = x_n)$$

- The $Y_i$ exhibit a Markov property where the  $Y_k^{th}$ observation can be solely predicted using the $k^{th}$ state of the $X_i$, for some arbitrary $k$. 
   + So, $Y_i$ observations are solely dependent on the state of the hidden $X_i$ and nothing else. Implying that
   
   $$P(Y_{n} \mid Y_1 = y_1, ..., Y_{n} = y_n, \hspace{.1 in} X_1 = x_1, \dots, X_n= x_{n}) = P(Y_{n}\mid X_{n} = x_{n})$$

 
##### Hidden States: {-}

- **Hidden States:** The unobservable possible outcomes of $X_i$. Denoted as $x_1, \dots, x_{n-1}, x_{n}$.
- Together, the outcomes form the **Hidden State Space**, which we will denote as $\mathbb{S}$.

##### Transition, Emission, and Initial Probabilities: {-}

- **Transition Probability:** The probability of changing $X_i$-states. These are typically placed into a transition matrix.
  + We typically denote this transition matrix as $\mathbb{a}_{ij}$, where we change from state $i$ to state $j$.


- **Emission Probability:** The probability of $Y_i$, given the $X_i$'s state.
  + So, $P(Y_{n}\mid X_{n} = x_{n})$ is the **Emission Probability** which can be placed into the **Emission Matrix**
    + We typically denote these emission probabilities as $b_n$, where $n$ is the state of the $X_i$.
  
- **Initial Probability:** An estimation of $X_i$'s state at some arbitrary point $a$. This is often denoted by $\pi$ and is typically as a start guess.

> **NOTE:** The above definitions were cumulatively drawn from the following sources ^[https://medium.com/@kangeugine/hidden-markov-model-7681c22f5b9] ^[https://jwmi.github.io/ASM/5-HMMs.pdf] ^[https://web.stanford.edu/~jurafsky/slp3/A.pdf]

### Example: Frogs {-} 

Let's formalize the frog-family's situation, $\mathbb{F}$, with the above definitions! 

Let $W_i = W_1, \dots , W_n$ be the weather for $n$  days. We know from previous work that the weather is a Markov Chain. So, for the frogs, the **hidden states** of $W_i$ are sunny, rainy, and cloudy, while the hidden state space will be $\mathbb{W}$.

From the previous sections, we also know that the transition matrix of $W_i$ is

| |Rainy|Sunny|Cloudy|
|:---:|:---:|:---:|:----:|
|**Rainy**  | .20 | .46 | .34 |
|**Sunny**  | .00 | .80 | .20 |
|**Cloudy** | .15 | .50 | .30 |


And let our **initial probabilities** ($\pi$) of $W$ be

|Rainy|Sunny|Cloudy|
|:---:|:---:|:----:|
| .2 | .5 | .3 |


\

Finally, let $D_i = D_1, \dots, D_n$ be whether or not we see a frog die after leaving the tree. We know that the **emission probabilities** of the $D_i$s are therefore

| |$\bf{P(D_i\mid \text{Rainy})}$|$\bf{P(D_i\mid \text{Sunny})}$|$$\bf{P(D_i\mid \text{Cloudy})}$$|
|:---:|:---:|:---:|:----:|
|**Survives**  | .98 | .10 | .75 |
|**Dead**  | .02 | .90 | .25 |

> **NOTE:** These are actually 3 separate emission matrices squashed together for simplicity. The columns contain the desired matrices.

In the next section, we'll show some visual examples of this frog family's HMM,  $\mathbb{F}$.

### Visualizations {-}

**Frog Graphs**

Below, we've drawn the Hidden Markov Model, $\mathbb{F}$ & placed it into a Weighted Directed Graph.

![Fig 2. HMM Directed Graph. Made by Author](images/HMM_Diagram_Frog.png)

\

**Frog Animations**

We can reuse our previous animation to simulate state-space transitions and what that implies for our frog family's chance of living. 

Now, the dots change colors with the true but unobserved weather of each day. Written text now describes,

- The emission probability of dying on a given day.
- The number of unobserved, but true, weather states.

```{r, warning=FALSE, error=FALSE, message=FALSE, echo = FALSE, fig.align = "center"}
library(plotly)
library(tidyverse)
set.seed(200322)
markov2 <- function(n = 1, iter = 31, start_probs = c(.3, .5, .2), 
                    trans_probs = matrix(c(.2, 0, .15, 
                                           .46, .8, .55,
                                           .34, .2, .30), ncol = 3),
                    plot_prob = TRUE) { # Here we have a full non-zero transition matrix.
  #Check to see if probability entries are valid.
  if(sum(start_probs) != 1 | any(start_probs < 0)) 
    stop("start_probs must be non-negative and sum to 1.")
  if(any(apply(trans_probs, 1, sum) != 1))
    stop("trans_probs matrix rows must sum to 1.")
  if(any(trans_probs < 0))
    stop("elements of trans_probs must be non-negative")

  dt <- matrix(NA, nrow = iter, ncol = n) # Initialize matrix to hold iterations
  # Run chain
  for(i in 1:iter) {
    for(j in 1:n) {
      if(i == 1) { # if we're at the beginning of the simulation
        dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = start_probs)
      } else {
        if(dt[i - 1, j] == 0) { # if the previous state was 0
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,1])
        } else if (dt[i - 1, j] == 1) { # if the previous state was 1
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,2])
        } else {
          dt[i, j] <- sample(x = c(0, 1, 2), size = 1, prob = trans_probs[,3])
        }
      }
    }
  }
  
  # Return chain as dataframe
  return(as.data.frame(dt))
}

df2 <- markov2()
 
df_long2 <- df2 %>%
  rowid_to_column(var = "iter") %>%
  pivot_longer(cols = V1) %>%
  group_by(name) %>%
  mutate(x = value + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         y = value %% 2 + rnorm(1, 0, .10) + rnorm(n(), 0, .01),
         initial = value[1],
         is_0 = ifelse(value == 0, TRUE, FALSE),
         is_1 = ifelse(value == 1, TRUE, FALSE)) %>%
  group_by(iter) %>%
  mutate(prop0 = mean(is_0),
         prop1 = mean(is_1),
         prop2 = 1 - prop0 - prop1) %>%
  ungroup()

df_long2 <- df_long2 %>%
  mutate(x = case_when(
    x < .75 ~ 0,
    x > .75 & x < 1.5 ~ 1,
    x > 1.5 ~ 2,
    TRUE ~ x
  )) %>%
  mutate(y = case_when(
    y < 0 ~ 0,
    y > 0 ~ 1,
    TRUE ~ y
  ))%>%
  mutate(prop0 = case_when(
    y < 0 ~ 0,
    y > 0 ~ 1,
    TRUE ~ y
  ))
library(openxlsx)
#write.xlsx(df_long2, file = "/Users/freddy/Desktop/df_long2.xlsx")
df_long2 <- read.xlsx("data/df_long2_standard.xlsx")

anim2 <- df_long2 %>%
  plot_ly(
    x = ~x,
    y = ~y,
    color = ~factor(value),
    size = 40,
    colors =c("#FAD77B" , "#85D4E3",  "#D5D5D3" ),
    frame = ~iter,
    type = 'scatter',
    mode = 'markers',
    showlegend = FALSE
  )

anim2 <- anim2 %>%
  add_text(x = 0.2, y = .35, text = ~prop0, textfont = list(color = "#FAD77B", size = 24, opacity = .6)) %>%
  add_text(x = 1, y = 1.25, text = ~prop1, textfont = list(color = "#85D4E3", size = 24, opacity = .6)) %>%
  add_text(x = 1.8, y = .35, text = ~prop2, textfont = list(color = "#D5D5D3", size = 24, opacity = .6)) %>%
  add_text(x=1, y=1.5, text = ~Chance, textfont = list(color = "black", size = 24))


ax2 <- list(
  zeroline = TRUE,
  showline = TRUE,
  mirror = "ticks",
  showticklabels = FALSE,
  gridcolor = toRGB("white"),
  gridwidth = 2,
  zerolinecolor = toRGB("white"),
  zerolinewidth = 4,
  linecolor = toRGB("black"),
  linewidth = 2,
  title = "",
  automargin = TRUE
)

anim2 <- anim2 %>%
  layout(xaxis = ax2, yaxis = ax2) %>%
  animation_opts(redraw = FALSE) %>%
  animation_slider(hide = TRUE) %>%
  animation_button(x = .6, y = .10, showactive = FALSE, label = "Run Simulation") %>%
  config(displayModeBar = FALSE, scrollZoom = FALSE, showTips = FALSE)



  
anim2


```

What a nice way of looking at the relationship between emission probabilities and the hidden states! In this next section, we'll see how we model the hidden states using observations.  


\

---


## Algorithms

Let's say the frog family has been really lucky, and all the frogs who left the tree hide in the past week came back! One very curious frog named Betsy wonders what the probability of all her family surviving is, given the weather.

We can answer Betsy's questions using likelihood calculations or the Forward-Backward Algorithm.

### Likelihood{-}

**Likelihood** or **Posterior Probability** are commonly defined as the probability of the data, given a true underlying parameter^[https://www.bayesrulesbook.com]. 

Let $W_i$ be the hidden states of the weather in the past week, and let $D_i$ be our observations on whether or not the frogs came back alive. This implies

\begin{align*}
W_i &= \{W_1= w_1, W_2=w_2, W_3=w_3,W_4= w_4, W_5=w_5, W_6=w_6, W_7=w_7\}\\
W_i &= \{w_1, w_2, w_3, w_4, w_5, w_6, w_7\} \\[15pt]
\text{and}\\[15pt]
D_i &= \{D_1= d_1, D_2=d_2, D_3=d_3,D_4= d_4, D_5=d_5, D_6=d_6, D_7=d_7\\
D_i &= \{d_1, d_2, d_3, d_4, d_5, d_6, d_7\}
\end{align*}

In this case, the likelihood would be the probability of our observed $D_i$, given some true underlying set of $W_i$. This is computed below.

\begin{align*}
P(D_i \mid W_i) &= P(d_1 \mid w_1)  \times  P(d_2 \mid w_2) \\ 
& \times  P(d_3 \mid w_3)  \times  P(d_4 \mid w_4) \\ &  \times  P(d_5 \mid w_5)  \times  P(d_6 \mid w_6) \\
& \times P(d_7 \mid w_7) \\
P(D_i \mid W_i) &= \prod_{i=1}^7 P(d_i \mid w_i)
\end{align*}

> **NOTE:** The capital pi ($\prod$) means to multiply a series indexed by $i$.

\

If we knew the states of the past week were `Sunny`, `Cloudy`, `Rainy`, `Rainy`, `Rainy`, `Rainy`, `Rainy` and everyone came back alive. We'd compute the likelihood as

\begin{align*}
P(D_i \mid W_i) &= P(Lives \mid Sunny) \times P(Lives \mid Cloudy)\\
&\times P(Lives \mid Rainy)\times P(Lives \mid Rainy) \\
&   \times P(Lives \mid Rainy) \times P(Lives \mid Rainy) \\
& \times P(Lives \mid Rainy) \\[10pt]
P(D_i \mid W_i) &= .10 \times .75 \times .98  \times .98  \times .98  \times .98  \times .98  \\
P(D_i \mid W_i) &= 0.06779406  \\
P(D_i \mid W_i) &\approx 0.068  \\
\end{align*}

\

**Bad News:** We can't truly know the states of the $W_i$! Bummer.

Because of this, we would need to add together all likelihoods for all possible states for each event, weighted by their probability^[https://web.stanford.edu/~jurafsky/slp3/A.pdf] . More succinctly, we intend to find the likelihood of all observations $P(D_i)$, given the probability of all states $P(D_i \cap W_i)$. We can do this with the rules of joint probability we covered in the previous section.

\begin{align*}
P(D_i \cap W_i) &= P(D_i \mid W_i)\times P(W_i) \\
&= \prod_{i=1}^7 P(d_i \mid w_i) \times P(W_i) && \text{by above}\\
&= \prod_{i=1}^7 P(d_i \mid w_i) \times \prod_{i=1}^7 P(w_i \mid w_{i-1}) && \text{by Markov}
\end{align*}

Now, we sum up all the observations given.

$$P(D_i) = \sum_{i=1}^n P(D_i \cap W_i) = \sum_{i=1}^n P(D_i \mid W_i)\times P(W_i)$$

So, for our 7 consecutive live frogs, this would be 

\begin{align*}
P(D_i) &=P(Lives \mid Sunny) \times P(Lives \mid Sunny)\times P(Lives \mid Sunny)\\
&\times P(Lives \mid Sunny)\times P(Lives \mid Sunny)  \times P(Lives \mid Sunny)  \times P(Lives \mid Sunny) \\[10pt]
& + P(Lives \mid Cloudy) \times P(Lives \mid Sunny)\times P(Lives \mid Sunny)\\
&\times P(Lives \mid Sunny)\times P(Lives \mid Sunny)  \times P(Lives \mid Sunny) \times P(Lives \mid Sunny) \\[10pt]
& + P(Lives \mid Sunny) \times P(Lives \mid Cloudy)\times P(Lives \mid Sunny)\\
&\times P(Lives \mid Sunny)\times P(Lives \mid Sunny)  \times P(Lives \mid Sunny)  \times P(Lives \mid Sunny) \\[10pt]
& + P(Lives \mid Sunny) \times P(Lives \mid Sunny)\times P(Lives \mid Cloudy)\\
&\times P(Lives \mid Sunny)\times P(Lives \mid Sunny)  \times P(Lives \mid Sunny)  \times P(Lives \mid Sunny) \\[10pt]
&+ \dots \\[10pt]
&=P(Lives \mid Rainy) \times P(Lives \mid Rainy)\times P(Lives \mid Rainy)\\
&\times P(Lives \mid Rainy)\times P(Lives \mid Rainy)  \times P(Lives \mid Rainy)  \times P(Lives \mid Rainy) \\[10pt]
\end{align*}

That is *extremely difficult* to do when we the number of our states ($N$) are numerous or we have some arbitrarily large number observations, $T$, since we'd be analyzing $N^T$ different possible sequences. However, we can predict the hidden states of HMMs like this one, using the Forward-Backward Algorithm.

### The Forward-Backward Algorithm {-}

The Forward-Backward Algorithm is a dynamic programming algorithm used to infer the probability of seeing observations in a Hidden Markov Model. It contains two sub-algorithms, the Forward Algorithm & the Backward Algorithm, which respectively compute the probability of the data from the beginning and the end of observations until they collide and result in a total probability of the data^[https://web.stanford.edu/~jurafsky/slp3/A.pdf]. The process may seem extra, but as you've seen in the last section's toy computation, we have to be extra out of necessity.

> **NOTE:** The following sections heavily drawn from the following sources  ^[https://web.stanford.edu/~jurafsky/slp3/A.pdf]  ^[https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf]  ^[https://www.cs.tut.fi/kurssit/SGN-24006/PDF/L08-HMMs.pdf]  ^[http://www.columbia.edu/~mh2078/MachineLearningORFE/HMMs_MasterSlides.pdf]

#### Forward {-}

In the first section of the Forward-Backward Algorithm, we compute the **forward probabilities**. Forward probabilities are informally the joint probability of all observations so far and the probability of being in some given state space.

Formally, a forward probability is defined as
$$\alpha_k(j) = P(d_1, \dots , d_k,w_k = j \mid \mathbb{F}) $$
> **NOTE:** That is, we compute the probability of the first $k$ observations ($d_1,\dots, d_k$) leading us to our current observation while also being in state $j$,  $\alpha_k(j)$.

To compute the forward probabilities, we must 

- Sum over all previous forward probabilities that would lead us to be in $\alpha_k(j)$ 
- Multiply by the transition probabilities $\mathbb{a}_{ij}$ describing the change from the previous state $i$ to the current state $j$. 
- Multiply again by the emission probabilities of our current observation $d_k$, given the state $j$. That is $b_j(d_{k})$.

Formally, this is written as

$$\alpha_{k}(j) = \left[\sum_{i=1}^\mathbb{W} \alpha_{k-1}(i) \mathbb{a}_{ij} \right]b_j(w_{k})$$

> **Note:** Remember that $\mathbb{W}$ is the state space!

\

In practice, we typically use recursion to compute the probability of our observations. Like so

1.) **Base Case:** We use the initial probability of our system's states.

\begin{align*}
\alpha_1(j) = \pi_jb_j(d_1) && \text{for } 1 \leq j \leq \mathbb{W}
\end{align*}

2.) **Recursion:** We use the previous case & transition & emission probabilities to compute the probability of being in the newest state $j$.

\begin{align*}
\alpha_{k}(j) = \left[\sum_{i=1}^\mathbb{W} \alpha_{k-1}(i) \mathbb{a}_{ij} \right]b_j(d_{k}) && \text{for } 1 \leq j \leq \mathbb{W}, 1 \leq k \leq n
\end{align*}


2.) **End:** We can then define the probability of the observations as the recursive sum of all forward probabilities.

$$P(D_{1:n} = d_{1:n} \mid \lambda) = \sum_{i=1}^\mathbb{W} \alpha_{n}(i)$$


#### Backward {-}

In the second section of the Forward-Backward Algorithm, we compute the **backward probabilities**. Backward probabilities are informally the joint probability of successive observations and the probability of being in some arbitrary state $v$, given that we start an observation $k+1$.


Formally, a backward probability is defined as
$$\beta_k(v) = P(d_{k+1}, \dots , d_n, \mid w_k = v , \mathbb{F}) $$



To compute the backward probabilities, we must 
- Sum the successive values after $\beta_{k+1}$ until the end of the k-observations at $d_n$.
- Multiply by the transition probabilities $\mathbb{a}_{vj}$ describing the change from the given current state $v$ to the next state $j$. 
- Multiply again by the emission probabilities of the success observation $d_{k+1}$. That is $b_j(d_{k+1})$.

Formally, this is written as

$$\beta_k(v) = \sum_{j=1}^\mathbb{W} \mathbb{a}_{vj}b_j(d_{k+1}) \beta_{k+1}(j) $$

> **Note:** Remember that $\mathbb{W}$ is the state space!

\

In practice, we typically use recursion to compute the probability of our observations. Like so

1.) **Base Case:** We use the initial probability of our system's states. The value is 1 since we're working backward!

\begin{align*}
\beta_n(i) = 1 && \text{for } 1 \leq v \leq \mathbb{W}
\end{align*}

2.) **Recursion:** We use the previous transition & emission probabilities of being in the previous state $i$ from the state $j$ to weigh the sum of all success values.

\begin{align*}
\beta_k(v) = \sum_{j=1}^\mathbb{W} \mathbb{a}_{vj}b_j(d_{k+1}) \beta_{k+1}(j) && \text{for } 1 \leq v \leq \mathbb{W}, 1 \leq k \leq n
\end{align*}


2.) **End:** We can then define the backward probability of the observations as the recursive sum of values after $k$, beginning with the end value.

$$P(D_{k+1:n} =d_{k+1:n} \mid \lambda) = \sum^\mathbb{W}_{j=1}\pi_j b_j(d_1)\beta_1(j)$$
In the end, we've done the same thing! But now, we can use the probabilities of our observations to study the distribution of the hidden states.

#### Collision {-}

We can use Bayes' Rule, joint probability, and our previous work to compute the probability of being in a state $v$ at a given time, $t$. This is commonly called a **smoother** analysis. Like before, we will use the notation of our frog example to describe the states ($W_i$) and observations ($D_i$).

By Bayes' Rule, the probability of being in state, $v$, given the observations, is 

$$\gamma_t(v) =  P(W_t = v \mid D_i) = \frac{P(W_i \cap D_i )}{P(D_i)} $$

However, we can make this simpler by acknowledging that $P(W_i \mid D_i)$ is proportional (e.g., division by a constant value) to the joint probability of $W_i$ and $D_i$. So,

\begin{align*}
\gamma_t(v)  &\propto P(W_i \cap D_i) \\
\end{align*}

We can simplify this even further by splitting our $n$ observations at the $k^th$ observation, like 

- $D_{i} = \{d_1, \dots, d_n\}$
- $D_{1:k} = \{d_1, \dots, d_k\}$
- $D_{k+1:n} = \{d_{k+1}, \dots, d_{n}\}$

Then, 

\begin{align*}
\gamma_t(v)  &\propto P(W_i \cap D_i) \\
&\propto P(D_{k+1:n} \mid W_i \cap D_{1:k}) \times P(W_i  \cap D_{1:k}) && \text{by Bayes Rule}\\
&\propto P(D_{k+1:n} \mid W_i) \times P(W_i  \cap D_{1:k}) && \text{by Markov Property of } D_i\\
\end{align*}

We recognize these as the backward probability times the forward. We include our constant value $\frac{1}{ \sum^\mathbb{W}\beta_t(v) \times \alpha_t(v)}$.

\begin{align*}
\gamma_t(v) =  P(W_t = v \mid D_i) = \text{backward} \times \text{forward} \times \frac{1}{ \sum^\mathbb{W} \beta_t(v) \times \alpha_t(v)}
\end{align*}

> **Note:** We've multiplied by the constant value of $\frac{1}{ \sum^\mathbb{W}\beta_t(v) \times \alpha_t(v)}$ because the marginal distribution of the $D_i$ would be the sum of the probabilities of the observations for each possible state, by the Law of Total Probability.

Thus, the probability of being in a given state $v$, at a particular time, $t$, given the data is

$$\gamma_t(v) =  P(W_t = v \mid D_i) = \frac{\beta_t(v) \times \alpha_t(v)}{\sum^\mathbb{W} \beta_t(v) \times \alpha_t(v)}$$

We did it! Intuitively, it might not make sense why we can compute the probability of being at a state by multiplying the backward & forward probabilities, then dividing by the possible states. 

The idea is that the recursive functions "scan" through the observations, determining some probability of the next or previous outcome, until they *collide* and essentially collapse onto a probability of that observation. They do this for each possible observation. Then, by dividing the total probability of the observations, we can get the probability of the $v$-state.



## Conclusion 

We did it! We've learned how to compute the probability of the observations & the probability of a particular state, given a time, using the Forward-Backward Algorithm. There are, of course, other problems that Hidden Markov Models can solve, like finding the most probable sequence of *all* states or how to maximize the functions forward or backward functions. 

However, the Forward-Backward Algorithm has *so* many meaningful applications throughout the natural & social sciences alone. We'll look at a couple in the next chapter.

If you have any lingering questions, I've linked some great YouTube videos that may be helpful below.

## Video Resources {-}

**Hidden Markov Model Properties**

<center>
<iframe width="280" height="157.5" src="https://www.youtube.com/embed/5araDjcBHMQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="280" height="157.5" src="https://www.youtube.com/embed/_y0xTiPa2MY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>


**Forward Backward & Likelihood**

<center>
<iframe width="280" height="157.5" src="https://www.youtube.com/embed/gYma8Gw38Os" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>

\


## References {-}



<!--chapter:end:02-Hidden-Markov-Models.Rmd-->

# Application to Statistical Genetics

The goals of epidemiology explicitly revolve around studying the incidence, distribution, and control of disease. As one can imagine, the boundaries of the field are highly nebulous. So far, we've looked at Hidden Markov Models as mathematical objects devoid of any real context— aside from a toy frog example. This section intends to tie together the mathematical principles of HMMs with their connections to big areas in biostatistical & epidemiological research. We find that, yes, HMMs *are* cool because they can help save lives, bridge the gaps across survival disparities, and study our bodies.

\

---

## Local Ancestry Inference

Local Ancestry Inference (LAI) is an approach in statistical genetics & bioinformatics that aims to study genetic diversity, disease prevalence, or evolutionary development by linking each segment of a genome to some ancestral population^[http://cs229.stanford.edu/proj2015/290_report.pdf]. LAI has been used to study crop plant analysis^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7003078/], Tilapia breeding^[https://www.nature.com/articles/s41598-020-75744-9], and the genetic adaptations of cows in East Africa^[https://www.frontiersin.org/articles/10.3389/fgene.2014.00443/full]. However, using LAI in public health studies is a critically important step when studying ethnically diverse cohorts.

Diasporic movement, mass migration, colonial empire, and slavery have profoundly changed the genetic landscape of the human population. Prior to these *admixture events*, human communities were largely isolated from each other, and over time, these isolated communities genetically diverged into human sub-populations. The differences between sub-populations are largely minuscule. While the differences are mostly nonexistent, generations of independent breeding did allow for the development of genes & mutations that are more common in some sub-populations rather than others^[https://pubmed.ncbi.nlm.nih.gov/20594047/]. 

> **NOTE:** Ancestry & Race are two different concepts that do not measure the same thing in the same way.

Admixed populations are products of the intense migratory & diasporic events that marked the 14th-19th centuries, which resulted in populations whose genetic ancestries were from 2 or more human sub-populations. We call these ancestrally-mixed populations **admixed** populations. Notable examples of admixed populations include African Americans, Latines, Ashkenazi Jews, and SWANA peoples. However, admixture is everpresent across most global populations. For example, this plot made by Hodgson et al. shows the admixture proportion across 81 different African populations with colorings for 10, 11, and 12 respective common ancestries ^[http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004393].

\

<center>
![Figure 1. Admixture proportions of Horn of Africa populations (Hodgson et al.)](images/map.png)

</center>

\

Unfortunately, the violence of diaspora, colonialism, and enslavement also brought the dangers of inheriting potentially dangerous mutations by family, ancestry, or ethnicity. For example, the inheritance of Tay-Sachs, Ellis-van Creveld Syndrome, diabetes, and prostate cancer all occur with strong associations to ancestry^[https://pubmed.ncbi.nlm.nih.gov/20594047/]. 

So, when studying disease with evidence of some ancestral association, LAI allows public researchers to first map the ancestry of an individual's genome, before going on to find associations between disease & ancestral lineage. 





### Local Ancestry {-}

Local Ancestry is formally defined as "the genetic ancestry of an individual at a particular chromosomal location, where an individual can have 0, 1, or 2 copies of an allele derived from each ancestral population.^[https://onlinelibrary.wiley.com/doi/abs/10.1002/gepi.21819]" 

Due to the structure of gene inheritance, we can model local ancestry as a Hidden Markov Model, with the observed! This idea has been formalized throughout the literature on statistical genetics, but we will look at one particular approach of using HMMs in the software HAPMIX to infer local ancestry in admixed populations.


### HAPMIX {-}

> **NOTE:** Most content in this section originates from HAPMIX's official release paper by Price et al.^[https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1000519].


**Definitions**

- A haplotype is a gene that is solely inherited from one parent and thus one parental population. Haplotypes are generally considered good markers of ancestral lineage.


**Assumptions:**

- We assume that the admixed population comes from only two ancestral populations
- We have access to reference data derived from unadmixed reference populations
- The unadmixed reference populations are closely related to the true ancestral populations.

**Process**

1) At every locus, HAPMIX estimates the likelihood that a "parental" haplotype from an admixed individual comes from a referenced ancestral population.

2) Loci that are next to each other are more likely to come from the same ancestral lineage^[due to a concept called Linkage Disequilibrium], but that doesn't hold true after enough positions. So, we say that the number of genes inherited from an ancestral population at the $j^th$ locus can be strictly predicted by the ancestry of the ${j-1}^{th}$ locus.

3) We can then use an HMM where the hidden states are the number of ancestral genes inherited $(0,1,2)$ at locus $j$, and the observations are the likelihoods of observing some mutation at locus $j$, given the ancestral reference panel.

4) Using a smoother analysis, similar to the Forward-Backward Algorithm, we can combine these likelihoods to provide a probabilistic estimate of ancestry at each locus. 

**Visual**

Below is a visual made by Price et al. describing the HMM employed in HAPMIX.

<center>

![Fig 2. Markov Schematic (Price et al.)](images/hapmix.png)

</center>


- The thick bottom bar represents the true 2-state ancestral composition of an individual, and the dotted line above it represents the ancestral reference panel, from which we compute likelihoods.
- The line line represents a chromosomal segment of an individual, while the black circles represent typed mutations sites that can be distinguished between the ancestries.
- When the yellow line crosses a red or blue line, it is visualizing the selection of an ancestral group, given the individual's observed chromosome. Notice that when the line crosses through a dot on one particular side, it will choose the side with equal or greater dots that are vertically aligned.
- The yellow line can misclassify the ancestries since this is a probabilistic model. Notice the portion of blue dots above the red bar.


### Conclusion

LAI is only one application of HMMs. Variations of HMMs can be used to study the differential use of condoms for HIV prevention^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3649016/], the race-differences in the progression of HIV for vulnerable, infected, and AIDS positive people between African American & White People^[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4023279/], or even when automatically monitoring the development of Influenza outbreaks^[https://link.springer.com/chapter/10.1007/978-3-540-45231-7_48]. 

This is only a primer (with a de-emphasis on application), but I absolutely suggest reading through the footnote references to so how cool HMMs become when using them to find ways of helping people, treating illness, and more!


\

## References {-}
 
 
 
 
 
 

<!--chapter:end:03-Application-to-Statistical-Genetics.Rmd-->

